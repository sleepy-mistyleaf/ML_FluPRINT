{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d60ef3e7",
   "metadata": {},
   "source": [
    "# Refining the Best Model: Hyperparameter Tuning and Ensemble Enhancements\n",
    "\n",
    "Having identified the best-performing model in our previous analyses, XGBoost combined with median imputation and ADASYN oversampling, we now focus on further improving its performance. \n",
    "\n",
    "This phase involves:\n",
    "\n",
    "- **Hyperparameter Tuning:**  \n",
    "  Systematic exploration of model and sampling parameters using both randomised and grid search strategies to find the optimal configuration.\n",
    "\n",
    "- **Ensemble Methods:**  \n",
    "  Incorporating bagging techniques like `BalancedBaggingClassifier` and `BaggingClassifier` around XGBoost to enhance robustness and reduce variance.\n",
    "\n",
    "- **Dimensionality Reduction:**  \n",
    "  Integrating Principal Component Analysis (PCA) to reduce feature space complexity, potentially improving generalisation.\n",
    "  \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4250bc7d",
   "metadata": {},
   "source": [
    "## Step 1 Import data, aggregate and filter\n",
    "\n",
    "As per our last workflows, we import the necessary data, aggregate and filter data, reay for ML training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cfd9a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "from collections import Counter\n",
    "\n",
    "# Scikit-learn components\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import make_scorer, f1_score, classification_report, roc_auc_score, log_loss\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "# Imbalanced-learn components\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from imblearn.ensemble import BalancedBaggingClassifier\n",
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "# XGBoost classifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Utilities\n",
    "import warnings\n",
    "from sklearn.base import clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "303fead2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the FluPRINT database CSV file\n",
    "database = r\"C:\\Users\\ \\OneDrive\\Documents\\Applied Data science\\FluPRINT_database\\FluPRINT_filtered_data\\Fluprint_cleaned.csv\"\n",
    "\n",
    "# Read the CSV file\n",
    "fluprint_filtered = pd.read_csv(database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cba4b902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pivoted data shape: (292, 3283)\n",
      "Original feature count: 3283\n",
      "Filtered feature count (<=90% missing): 407\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Aggregate duplicated measurements by donor and feature (mean)\n",
    "agg_df = fluprint_filtered.groupby([\"donor_id\", \"name_formatted\"], as_index=False)[\"data\"].median()\n",
    "\n",
    "# Step 2: Pivot to wide format - donors as rows, features as columns\n",
    "X_features = agg_df.pivot(index=\"donor_id\", columns=\"name_formatted\", values=\"data\")\n",
    "\n",
    "# Extract vaccine_response per donor\n",
    "y = fluprint_filtered.groupby(\"donor_id\")[\"vaccine_response\"].first()\n",
    "\n",
    "# Align y to X_features indices (donor_ids)\n",
    "y = y.loc[X_features.index]\n",
    "\n",
    "# Print shape of the pivoted data\n",
    "print(f\"Pivoted data shape: {X_features.shape}\")\n",
    "\n",
    "# Step 3: Calculate missing data fraction and drop high-missingness features\n",
    "missing_fraction = X_features.isnull().mean()\n",
    "keep_features = missing_fraction[missing_fraction <= 0.90].index\n",
    "X_filtered = X_features[keep_features]\n",
    "\n",
    "print(f\"Original feature count: {X_features.shape[1]}\")\n",
    "print(f\"Filtered feature count (<=90% missing): {X_filtered.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7b72a6",
   "metadata": {},
   "source": [
    "## Pipeline 1: Hyperparameter Tuning\n",
    "\n",
    "We use `RandomizedSearchCV` to search the best important XGBoost parameters, including:\n",
    "\n",
    "- Number of trees (`n_estimators`)\n",
    "- Maximum tree depth (`max_depth`)\n",
    "- Learning rate (`learning_rate`)\n",
    "- Subsample ratios for row and feature sampling (`subsample`, `colsample_bytree`)\n",
    "\n",
    "\n",
    "### Evaluation Goals\n",
    "We aim to:\n",
    "\n",
    "- **Increase Sensitivity and Recall:**  \n",
    "  Better identify actual high vaccine responders.\n",
    "\n",
    "- **Maximise AUC Score:**  \n",
    "  Improve the model's ability to discriminate between high and low responders at various classification thresholds.\n",
    "\n",
    "- **Reduce Log-Loss:**  \n",
    "  Ensure the predicted probabilities are well calibrated and confident without overfitting.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2219d4a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ \\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\joblib\\externals\\loky\\backend\\context.py:136: UserWarning: Could not find the number of physical cores for the following reason:\n",
      "[WinError 2] The system cannot find the file specified\n",
      "Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n",
      "  warnings.warn(\n",
      "  File \"C:\\Users\\ \\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 257, in _count_physical_cores\n",
      "    cpu_info = subprocess.run(\n",
      "               ^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\subprocess.py\", line 548, in run\n",
      "    with Popen(*popenargs, **kwargs) as process:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\subprocess.py\", line 1026, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\subprocess.py\", line 1538, in _execute_child\n",
      "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'classifier__subsample': 0.8, 'classifier__n_estimators': 100, 'classifier__max_depth': 3, 'classifier__learning_rate': 0.01, 'classifier__colsample_bytree': 0.8}\n",
      "Best CV F1-score: 0.6517936036671294\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.75      0.60      0.67        40\n",
      "         1.0       0.41      0.58      0.48        19\n",
      "\n",
      "    accuracy                           0.59        59\n",
      "   macro avg       0.58      0.59      0.57        59\n",
      "weighted avg       0.64      0.59      0.61        59\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.75      0.60      0.67        40\n",
      "         1.0       0.41      0.58      0.48        19\n",
      "\n",
      "    accuracy                           0.59        59\n",
      "   macro avg       0.58      0.59      0.57        59\n",
      "weighted avg       0.64      0.59      0.61        59\n",
      "\n",
      "AUC Score: 0.5947\n",
      "Log-loss Score: 0.6542\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Split the data first\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_filtered, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Step 2: Define pipeline after creating train/test splits\n",
    "pipeline = ImbPipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"adasyn\", ADASYN(random_state=42)),\n",
    "    (\"classifier\", XGBClassifier(random_state=42, eval_metric=\"logloss\"))\n",
    "])\n",
    "\n",
    "# Step 3: Define hyperparameter search space\n",
    "param_grid = {\n",
    "    \"classifier__n_estimators\": [100, 200, 300],\n",
    "    \"classifier__max_depth\": [3, 5, 7],\n",
    "    \"classifier__learning_rate\": [0.01, 0.1, 0.2],\n",
    "    \"classifier__subsample\": [0.6, 0.8, 1.0],\n",
    "    \"classifier__colsample_bytree\": [0.6, 0.8, 1.0],\n",
    "}\n",
    "\n",
    "# Step 4: Setup cross-validation and scoring\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scorer = make_scorer(f1_score, average=\"weighted\")\n",
    "\n",
    "# Step 5: Setup RandomizedSearchCV\n",
    "search = RandomizedSearchCV(\n",
    "    pipeline, param_distributions=param_grid, n_iter=20,\n",
    "    scoring=scorer, cv=cv, verbose=2, random_state=42, n_jobs=-1\n",
    ")\n",
    "\n",
    "# Step 6: Fit on the training data only\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "# Step 7: Output best parameters and score\n",
    "print(f\"Best parameters: {search.best_params_}\")\n",
    "print(f\"Best CV F1-score: {search.best_score_}\")\n",
    "\n",
    "# Step 8: Evaluate final model on test data\n",
    "y_pred = search.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Probability estimates for the positive class\n",
    "y_pred_proba = search.predict_proba(X_test)[:, 1]  \n",
    "\n",
    "# Calculate AUC\n",
    "auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "# Calculate Log-loss\n",
    "logloss_score = log_loss(y_test, y_pred_proba)\n",
    "\n",
    "# Print classification report as before\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Print AUC and Log-loss\n",
    "print(f\"AUC Score: {auc_score:.4f}\")\n",
    "print(f\"Log-loss Score: {logloss_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de12b7a2",
   "metadata": {},
   "source": [
    "## Pipeline 2: Expanded Hyperparameter Search\n",
    "\n",
    "We expand our exploration of XGBoost hyperparameters by increasing the search space to include:\n",
    "\n",
    "- Model complexity controls such as `min_child_weight`, `gamma`\n",
    "- Regularisation parameters `reg_alpha`, `reg_lambda`\n",
    "- Number of trees increased, deeper trees, and varied learning rates\n",
    "\n",
    "The pipeline structure remains with median imputation, scaling, ADASYN, and XGBoost, but hyperparameter tuning through `RandomizedSearchCV` now samples from a significantly larger parameter space.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb5c5670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "Best parameters: {'classifier__subsample': 0.6, 'classifier__reg_lambda': 0.5, 'classifier__reg_alpha': 1, 'classifier__n_estimators': 200, 'classifier__max_depth': 7, 'classifier__learning_rate': 0.2, 'classifier__colsample_bytree': 0.6}\n",
      "Best CV F1-score: 0.6437368480716198\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.70      0.80      0.74        40\n",
      "         1.0       0.38      0.26      0.31        19\n",
      "\n",
      "    accuracy                           0.63        59\n",
      "   macro avg       0.54      0.53      0.53        59\n",
      "weighted avg       0.60      0.63      0.61        59\n",
      "\n",
      "AUC Score: 0.6171\n",
      "Log-loss Score: 0.8174\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Split the data into training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_filtered, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Step 2: Define the full machine learning pipeline.\n",
    "pipeline = ImbPipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"adasyn\", ADASYN(random_state=42)),\n",
    "    (\"classifier\", XGBClassifier(random_state=42, eval_metric=\"logloss\"))\n",
    "])\n",
    "\n",
    "# Step 3: Define the hyperparameter search space.\n",
    "param_grid = {\n",
    "    \"classifier__n_estimators\": [100, 200, 300],\n",
    "    \"classifier__max_depth\": [3, 5, 7],\n",
    "    \"classifier__learning_rate\": [0.01, 0.1, 0.2],\n",
    "    \"classifier__subsample\": [0.6, 0.8, 1.0],\n",
    "    \"classifier__colsample_bytree\": [0.6, 0.8, 1.0],\n",
    "    \"classifier__reg_alpha\": [0, 0.1, 0.5, 1],  # L1 Regularisation\n",
    "    \"classifier__reg_lambda\": [0, 0.1, 0.5, 1]  # L2 Regularisation\n",
    "}\n",
    "\n",
    "# Step 4: Set up cross-validation and the evaluation metric.\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scorer = make_scorer(f1_score, average=\"weighted\")\n",
    "\n",
    "# Step 5: Configure and run the Randomized Search.\n",
    "search = RandomizedSearchCV(\n",
    "    pipeline, param_distributions=param_grid, n_iter=50,\n",
    "    scoring=scorer, cv=cv, verbose=2, random_state=42, n_jobs=-1\n",
    ")\n",
    "\n",
    "# Step 6: Fit the search on the training data.\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "# Step 7: Output the results.\n",
    "print(f\"Best parameters: {search.best_params_}\")\n",
    "print(f\"Best CV F1-score: {search.best_score_}\")\n",
    "\n",
    "# Step 8: Evaluate the best model on the unseen test data.\n",
    "y_pred = search.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Predict probabilities for AUC and log-loss calculations\n",
    "y_pred_proba = search.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate and print AUC score\n",
    "auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "print(f\"AUC Score: {auc_score:.4f}\")\n",
    "\n",
    "# Calculate and print Log-loss score\n",
    "logloss_score = log_loss(y_test, y_pred_proba)\n",
    "print(f\"Log-loss Score: {logloss_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f318120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2187 candidates, totalling 10935 fits\n",
      "Best parameters from Grid Search: {'classifier__colsample_bytree': 0.9, 'classifier__learning_rate': 0.25, 'classifier__max_depth': 5, 'classifier__n_estimators': 80, 'classifier__reg_alpha': 1.5, 'classifier__reg_lambda': 0.7, 'classifier__subsample': 0.9}\n",
      "Best CV F1-score from Grid Search: 0.6703703332947654\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.69      0.72      0.71        40\n",
      "         1.0       0.35      0.32      0.33        19\n",
      "\n",
      "    accuracy                           0.59        59\n",
      "   macro avg       0.52      0.52      0.52        59\n",
      "weighted avg       0.58      0.59      0.59        59\n",
      "\n",
      "AUC Score: 0.6053\n",
      "Log-loss Score: 0.8384\n"
     ]
    }
   ],
   "source": [
    "# Assuming X_filtered and y are already defined\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_filtered, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Define pipeline\n",
    "pipeline = ImbPipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"adasyn\", ADASYN(random_state=42)),\n",
    "    (\"classifier\", XGBClassifier(random_state=42, eval_metric=\"logloss\"))\n",
    "])\n",
    "\n",
    "# Refined hyperparameter grid from previous randomized search results\n",
    "param_grid = {\n",
    "    \"classifier__n_estimators\": [80, 100, 120],\n",
    "    \"classifier__max_depth\": [4, 5, 6],\n",
    "    \"classifier__learning_rate\": [0.15, 0.2, 0.25],\n",
    "    \"classifier__subsample\": [0.7, 0.8, 0.9],\n",
    "    \"classifier__colsample_bytree\": [0.7, 0.8, 0.9],\n",
    "    \"classifier__reg_alpha\": [0.5, 1, 1.5],\n",
    "    \"classifier__reg_lambda\": [0.3, 0.5, 0.7]\n",
    "}\n",
    "\n",
    "# Setup cross-validation and scorer\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scorer = make_scorer(f1_score, average=\"weighted\")\n",
    "\n",
    "# Setup GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline, param_grid=param_grid,\n",
    "    scoring=scorer, cv=cv, verbose=2, n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit gridsearch on training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Output best parameters and CV score\n",
    "print(f\"Best parameters from Grid Search: {grid_search.best_params_}\")\n",
    "print(f\"Best CV F1-score from Grid Search: {grid_search.best_score_}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred = grid_search.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Predict probabilities for AUC and log-loss calculations\n",
    "y_pred_proba = grid_search.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate and print AUC score\n",
    "auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "print(f\"AUC Score: {auc_score:.4f}\")\n",
    "\n",
    "# Calculate and print Log-loss score\n",
    "logloss_score = log_loss(y_test, y_pred_proba)\n",
    "print(f\"Log-loss Score: {logloss_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6c65ed",
   "metadata": {},
   "source": [
    "## Pipeline 3: Hyperparameter Tuning with ADASYN Sampling Strategy\n",
    "\n",
    "We further refine the previous model by tuning the oversampling behavior of ADASYN alongside XGBoost hyperparameters.\n",
    "\n",
    "- **ADASYN Sampling Strategy as a Hyperparameter:**  \n",
    "  Instead of a fixed resampling ratio, we explore other sampling strategies (`0.5`, `0.75`, `1.0`, and `\"auto\"`) to find the optimal level of synthetic minority class generation.\n",
    "\n",
    "- **Minority-Class Focused Scoring:**  \n",
    "  We use the F1-score calculated specifically for the minority class as our evaluation metric during hyperparameter tuning. This ensures that the model prioritises sensitivity and precision for the underrepresented vaccine responders.\n",
    "\n",
    "- **Expanded XGBoost Parameter Search:**  \n",
    "  The hyperparameter grid covers a wider range of estimators, tree depths, learning rates, and feature subsampling settings.\n",
    "\n",
    "### Goals:\n",
    "\n",
    "- Enhance the model’s ability to correctly identify vaccine responders by fine-tuning the oversampling process.\n",
    "- Improve minority-class recall and precision, addressing the key challenge of class imbalance.\n",
    "- Maintain balance between model complexity and generalisation through careful parameter selection.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42de312d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ \\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\model_selection\\_validation.py:528: FitFailedWarning: \n",
      "65 fits failed out of a total of 250.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "65 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ \\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\ \\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ \\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\imblearn\\pipeline.py\", line 518, in fit\n",
      "    Xt, yt = self._fit(X, y, routed_params, raw_params=params)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ \\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\imblearn\\pipeline.py\", line 440, in _fit\n",
      "    X, y, fitted_transformer = fit_resample_one_cached(\n",
      "                               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ \\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\joblib\\memory.py\", line 312, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ \\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\imblearn\\pipeline.py\", line 1336, in _fit_resample_one\n",
      "    X_res, y_res = sampler.fit_resample(X, y, **params.get(\"fit_resample\", {}))\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ \\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\imblearn\\base.py\", line 202, in fit_resample\n",
      "    return super().fit_resample(X, y, **params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ \\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ \\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\imblearn\\base.py\", line 105, in fit_resample\n",
      "    output = self._fit_resample(X, y, **params)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ \\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\imblearn\\over_sampling\\_adasyn.py\", line 173, in _fit_resample\n",
      "    raise ValueError(\n",
      "ValueError: No samples will be generated with the provided ratio settings.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\ \\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\model_selection\\_search.py:1108: UserWarning: One or more of the test scores are non-finite: [0.37561264 0.37639649 0.41239476 0.39743167        nan        nan\n",
      " 0.47068929 0.39464883 0.43118286        nan        nan 0.36813063\n",
      " 0.39213673 0.39350122 0.36015433 0.4214524         nan 0.35291224\n",
      "        nan        nan 0.37551617        nan 0.39085196 0.38099218\n",
      " 0.37931221 0.4056171         nan        nan 0.41680305        nan\n",
      " 0.43902825 0.37795906 0.39337911 0.42187327 0.34926778 0.4091551\n",
      "        nan 0.47993498 0.3715873  0.42067586 0.38524129 0.42128406\n",
      "        nan 0.33974482 0.41002927 0.39653367 0.43507716 0.42698767\n",
      " 0.34136289 0.41902786]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'classifier__subsample': 0.8, 'classifier__n_estimators': 200, 'classifier__max_depth': 3, 'classifier__learning_rate': 0.01, 'classifier__colsample_bytree': 0.8, 'adasyn__sampling_strategy': 1.0}\n",
      "Best CV Minority F1-score: 0.4799349773033984\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.76      0.65      0.70        40\n",
      "         1.0       0.44      0.58      0.50        19\n",
      "\n",
      "    accuracy                           0.63        59\n",
      "   macro avg       0.60      0.61      0.60        59\n",
      "weighted avg       0.66      0.63      0.64        59\n",
      "\n",
      "AUC Score: 0.6013\n",
      "Log-loss Score: 0.6544\n"
     ]
    }
   ],
   "source": [
    "# 1. Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_filtered, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# 2. Define the pipeline\n",
    "pipeline = ImbPipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"adasyn\", ADASYN(random_state=42)),\n",
    "    (\"classifier\", XGBClassifier(random_state=42, eval_metric=\"logloss\"))\n",
    "])\n",
    "\n",
    "# 3. Define the refined hyperparameter search space\n",
    "param_grid = {\n",
    "    # ADASYN parameters\n",
    "    \"adasyn__sampling_strategy\": [0.5, 0.75, 1.0, \"auto\"],\n",
    "\n",
    "    # XGBoost parameters\n",
    "    \"classifier__n_estimators\": [100, 200, 300, 400, 500, 750],\n",
    "    \"classifier__max_depth\": [3, 5, 7],\n",
    "    \"classifier__learning_rate\": [0.01, 0.1, 0.2],\n",
    "    \"classifier__subsample\": [0.6, 0.8, 1.0],\n",
    "    \"classifier__colsample_bytree\": [0.6, 0.8, 1.0],\n",
    "}\n",
    "\n",
    "# 4. Define cross-validation and a custom scorer for the minority class F1-score\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scorer = make_scorer(f1_score, pos_label=1.0)\n",
    "\n",
    "# 5. Setup and run RandomizedSearchCV\n",
    "search = RandomizedSearchCV(\n",
    "    pipeline, param_distributions=param_grid, n_iter=50,\n",
    "    scoring=scorer, cv=cv, verbose=2, random_state=42, n_jobs=-1\n",
    ")\n",
    "\n",
    "# 6. Fit on the training data\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "# 7. Output best parameters and score\n",
    "print(f\"Best parameters: {search.best_params_}\")\n",
    "print(f\"Best CV Minority F1-score: {search.best_score_}\")\n",
    "\n",
    "# 8. Evaluate final model on test data\n",
    "y_pred = search.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Predict probabilities for AUC and log-loss calculations\n",
    "y_pred_proba = search.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate and print AUC score\n",
    "auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "print(f\"AUC Score: {auc_score:.4f}\")\n",
    "\n",
    "# Calculate and print Log-loss score\n",
    "logloss_score = log_loss(y_test, y_pred_proba)\n",
    "print(f\"Log-loss Score: {logloss_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a07e95",
   "metadata": {},
   "source": [
    "## Pipeline 4: Hyperparameter Fine-Tuning\n",
    "\n",
    "Following initial broad searches, this pipeline narrows the hyperparameter space to concentrate around previously identified strong configurations, including:\n",
    "\n",
    "- Fixing ADASYN’s sampling strategy to the best-performing \"auto\" setting.\n",
    "- Tuning tree complexity and regularisation parameters (`max_depth`, `gamma`, `min_child_weight`).\n",
    "- Refining learning rate and subsampling proportions.\n",
    "- Increasing the number of randomised search iterations to 75, improving the chance of finding an optimal combination.\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8f9ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 75 candidates, totalling 375 fits\n",
      "Best parameters: {'classifier__subsample': 0.8, 'classifier__n_estimators': 200, 'classifier__min_child_weight': 1, 'classifier__max_depth': 3, 'classifier__learning_rate': 0.005, 'classifier__gamma': 0.1, 'classifier__colsample_bytree': 0.8, 'adasyn__sampling_strategy': 'auto'}\n",
      "Best CV Minority F1-score: 0.5262379788695578\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.74      0.62      0.68        40\n",
      "         1.0       0.40      0.53      0.45        19\n",
      "\n",
      "    accuracy                           0.59        59\n",
      "   macro avg       0.57      0.58      0.57        59\n",
      "weighted avg       0.63      0.59      0.60        59\n",
      "\n",
      "AUC Score: 0.5829\n",
      "Log-loss Score: 0.6588\n"
     ]
    }
   ],
   "source": [
    "# 1. Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_filtered, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# 2. Define the pipeline\n",
    "pipeline = ImbPipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"adasyn\", ADASYN(random_state=42)),\n",
    "    (\"classifier\", XGBClassifier(random_state=42, eval_metric=\"logloss\"))\n",
    "])\n",
    "\n",
    "# 3. Define the refined hyperparameter search space\n",
    "param_grid = {\n",
    "    # ADASYN parameters (keeping the best-performing option)\n",
    "    \"adasyn__sampling_strategy\": [\"auto\"],\n",
    "\n",
    "    # XGBoost parameters (focused search around previous best)\n",
    "    \"classifier__n_estimators\": [200, 300, 400, 500, 750, 1000],\n",
    "    \"classifier__max_depth\": [2, 3, 4],\n",
    "    \"classifier__learning_rate\": [0.005, 0.01, 0.02],\n",
    "    \"classifier__subsample\": [0.8, 1.0],\n",
    "    \"classifier__colsample_bytree\": [0.6, 0.8, 1.0],\n",
    "\n",
    "    # Optional: Include new regularisation parameters for a more thorough search\n",
    "    \"classifier__gamma\": [0, 0.1],\n",
    "    \"classifier__min_child_weight\": [1, 3],\n",
    "}\n",
    "\n",
    "# 4. Define cross-validation and a custom scorer for the minority class F1-score\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scorer = make_scorer(f1_score, pos_label=1.0)\n",
    "\n",
    "# 5. Setup and run RandomizedSearchCV with increased iterations\n",
    "search = RandomizedSearchCV(\n",
    "    pipeline, param_distributions=param_grid, n_iter=75,\n",
    "    scoring=scorer, cv=cv, verbose=2, random_state=42, n_jobs=-1\n",
    ")\n",
    "\n",
    "# 6. Fit on the training data\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "# 7. Output best parameters and score\n",
    "print(f\"Best parameters: {search.best_params_}\")\n",
    "print(f\"Best CV Minority F1-score: {search.best_score_}\")\n",
    "\n",
    "# 8. Evaluate final model on test data\n",
    "y_pred = search.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Predict probabilities for AUC and log-loss calculations\n",
    "y_pred_proba = search.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate and print AUC score\n",
    "auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "print(f\"AUC Score: {auc_score:.4f}\")\n",
    "\n",
    "# Calculate and print Log-loss score\n",
    "logloss_score = log_loss(y_test, y_pred_proba)\n",
    "print(f\"Log-loss Score: {logloss_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cc4c1f",
   "metadata": {},
   "source": [
    "## Pipeline 5: Integrating PCA with ADASYN and XGBoost for Dimensionality Reduction\n",
    "\n",
    "In this pipeline, we introduce Principal Component Analysis (PCA) to reduce the feature space dimensionality before oversampling and classification.\n",
    "\n",
    "### Pipeline Components:\n",
    "\n",
    "- **Standard Scaling:** Normalising features for PCA and classifier consistency.\n",
    "- **PCA:** Reduces dimensions while preserving 95% of variance, potentially improving computational efficiency and generalisability.\n",
    "\n",
    "### Hyperparameter Tuning:\n",
    "\n",
    "We perform randomisd search over key XGBoost parameters such as number of estimators, maximum tree depth, learning rate, and subsampling ratios, with stratified 5-fold cross-validation.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e16d9f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Best parameters: {'classifier__subsample': 0.8, 'classifier__reg_lambda': 1.5, 'classifier__reg_alpha': 0.1, 'classifier__n_estimators': 200, 'classifier__min_child_weight': 5, 'classifier__max_depth': 5, 'classifier__max_delta_step': 5, 'classifier__learning_rate': 0.1, 'classifier__gamma': 0.1, 'classifier__colsample_bytree': 0.8}\n",
      "Best CV F1-score: 0.5779704904853005\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.62      0.62      0.62        40\n",
      "         1.0       0.21      0.21      0.21        19\n",
      "\n",
      "    accuracy                           0.49        59\n",
      "   macro avg       0.42      0.42      0.42        59\n",
      "weighted avg       0.49      0.49      0.49        59\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_filtered, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Calculate scale_pos_weight\n",
    "counter = Counter(y_train)\n",
    "scale_pos_weight = counter[0.0] / counter[1.0]\n",
    "\n",
    "# Pipeline with PCA added\n",
    "pipeline = ImbPipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"pca\", PCA(n_components=0.95)),\n",
    "    (\"adasyn\", ADASYN(random_state=42)),\n",
    "    (\"classifier\", XGBClassifier(random_state=42, eval_metric=\"logloss\", scale_pos_weight=scale_pos_weight))\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    \"classifier__n_estimators\": [100, 200, 300],\n",
    "    \"classifier__max_depth\": [3, 5, 7, 9],\n",
    "    \"classifier__learning_rate\": [0.01, 0.05, 0.1, 0.2],\n",
    "    \"classifier__subsample\": [0.6, 0.8, 1.0],\n",
    "    \"classifier__colsample_bytree\": [0.6, 0.8, 1.0],\n",
    "    \"classifier__min_child_weight\": [1, 3, 5],\n",
    "    \"classifier__gamma\": [0, 0.1, 0.3],\n",
    "    \"classifier__reg_alpha\": [0, 0.01, 0.1],\n",
    "    \"classifier__reg_lambda\": [1, 1.5, 2],\n",
    "    \"classifier__max_delta_step\": [0, 1, 5]\n",
    "}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scorer = make_scorer(f1_score, average=\"weighted\")\n",
    "\n",
    "search = RandomizedSearchCV(\n",
    "    pipeline, param_distributions=param_grid, n_iter=20,\n",
    "    scoring=scorer, cv=cv, verbose=2, random_state=42, n_jobs=-1\n",
    ")\n",
    "\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best parameters: {search.best_params_}\")\n",
    "print(f\"Best CV F1-score: {search.best_score_}\")\n",
    "\n",
    "y_pred = search.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead9b3e1",
   "metadata": {},
   "source": [
    "## Pipeline 6: Balanced Bagging Ensemble with XGBoost Base Estimator and Evaluation Metrics\n",
    "\n",
    "### Pipeline Components:\n",
    "- **Balanced Bagging:** Creates multiple balanced bootstrap samples and trains an XGBoost model on each, helping to reduce variance and bias from imbalanced data.\n",
    "\n",
    "### Hyperparameter Tuning:\n",
    "- Randomised search explores hyperparameters of both the bagging ensemble (e.g., number of base estimators and sampling strategies) and the XGBoost base learner (e.g., tree depth, learning rate, subsample ratios).\n",
    "- Stratified 5-fold cross-validation with weighted F1-score ensures balanced evaluation across classes.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ec4857d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ \\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\model_selection\\_validation.py:528: FitFailedWarning: \n",
      "70 fits failed out of a total of 250.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "70 fits failed with the following error:\n",
      "joblib.externals.loky.process_executor._RemoteTraceback: \n",
      "\"\"\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ \\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\joblib\\_utils.py\", line 72, in __call__\n",
      "    return self.func(**kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ \\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\joblib\\parallel.py\", line 598, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ \\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\utils\\parallel.py\", line 139, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ \\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\ensemble\\_bagging.py\", line 197, in _parallel_build_estimators\n",
      "    estimator_fit(X_, y_, **fit_params_)\n",
      "  File \"C:\\Users\\ \\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ \\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\imblearn\\pipeline.py\", line 518, in fit\n",
      "    Xt, yt = self._fit(X, y, routed_params, raw_params=params)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ \\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\imblearn\\pipeline.py\", line 440, in _fit\n",
      "    X, y, fitted_transformer = fit_resample_one_cached(\n",
      "                               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ \\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\joblib\\memory.py\", line 312, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ \\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\imblearn\\pipeline.py\", line 1336, in _fit_resample_one\n",
      "    X_res, y_res = sampler.fit_resample(X, y, **params.get(\"fit_resample\", {}))\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ \\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\imblearn\\base.py\", line 202, in fit_resample\n",
      "    return super().fit_resample(X, y, **params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ \\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ \\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\imblearn\\base.py\", line 101, in fit_resample\n",
      "    self.sampling_strategy_ = check_sampling_strategy(\n",
      "                              ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ \\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\imblearn\\utils\\_validation.py\", line 571, in check_sampling_strategy\n",
      "    _sampling_strategy_float(sampling_strategy, y, sampling_type).items()\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ \\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\imblearn\\utils\\_validation.py\", line 430, in _sampling_strategy_float\n",
      "    raise ValueError(\n",
      "ValueError: The specified ratio required to generate new sample in the majority class while trying to remove samples. Please increase the ratio.\n",
      "\"\"\"\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ \\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\ \\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ \\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\imblearn\\pipeline.py\", line 526, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **last_step_params[\"fit\"])\n",
      "  File \"C:\\Users\\ \\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ \\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\imblearn\\ensemble\\_bagging.py\", line 337, in fit\n",
      "    return super().fit(X, y)\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ \\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ \\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ \\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\ensemble\\_bagging.py\", line 389, in fit\n",
      "    return self._fit(X, y, max_samples=self.max_samples, **fit_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ \\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\imblearn\\ensemble\\_bagging.py\", line 352, in _fit\n",
      "    return super()._fit(X, y, self.max_samples)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ \\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\ensemble\\_bagging.py\", line 532, in _fit\n",
      "    all_results = Parallel(\n",
      "                  ^^^^^^^^^\n",
      "  File \"C:\\Users\\ \\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\utils\\parallel.py\", line 77, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ \\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\joblib\\parallel.py\", line 2007, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "                                                ^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ \\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\joblib\\parallel.py\", line 1650, in _get_outputs\n",
      "    yield from self._retrieve()\n",
      "  File \"C:\\Users\\ \\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\joblib\\parallel.py\", line 1754, in _retrieve\n",
      "    self._raise_error_fast()\n",
      "  File \"C:\\Users\\ \\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\joblib\\parallel.py\", line 1789, in _raise_error_fast\n",
      "    error_job.get_result(self.timeout)\n",
      "  File \"C:\\Users\\ \\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\joblib\\parallel.py\", line 745, in get_result\n",
      "    return self._return_or_raise()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ \\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\joblib\\parallel.py\", line 763, in _return_or_raise\n",
      "    raise self._result\n",
      "ValueError: The specified ratio required to generate new sample in the majority class while trying to remove samples. Please increase the ratio.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\ \\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\model_selection\\_search.py:1108: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.62938747        nan 0.605657   0.62829756\n",
      " 0.64254157        nan 0.60577127        nan        nan 0.62682452\n",
      " 0.6199054         nan 0.61972428        nan        nan 0.62057799\n",
      " 0.6192555         nan 0.60548254 0.62832858 0.61129798        nan\n",
      "        nan 0.62076134 0.6233256  0.6251323  0.61800686 0.63275376\n",
      " 0.60736312 0.61163831 0.63452024 0.63208127 0.60533736 0.62829756\n",
      " 0.63275376 0.61294419 0.61006985 0.6077495  0.60843189 0.649528\n",
      " 0.63976763 0.60495953 0.64254157 0.62493011        nan 0.65184106\n",
      " 0.61838679        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'bagging__sampling_strategy': 'not minority', 'bagging__n_estimators': 10, 'bagging__estimator__subsample': 0.8, 'bagging__estimator__n_estimators': 200, 'bagging__estimator__max_depth': 3, 'bagging__estimator__learning_rate': 0.05, 'bagging__estimator__colsample_bytree': 0.8}\n",
      "Best CV weighted F1-score: 0.6518410594912083\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.67      0.65      0.66        40\n",
      "         1.0       0.30      0.32      0.31        19\n",
      "\n",
      "    accuracy                           0.54        59\n",
      "   macro avg       0.48      0.48      0.48        59\n",
      "weighted avg       0.55      0.54      0.55        59\n",
      "\n",
      "AUC Score: 0.5947\n",
      "Log-loss Score: 0.6989\n"
     ]
    }
   ],
   "source": [
    "# Assume X_filtered and y are already defined\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_filtered, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Define base XGBoost model\n",
    "base_xgb_classifier = XGBClassifier(\n",
    "    random_state=42,\n",
    "    eval_metric=\"logloss\"\n",
    ")\n",
    "\n",
    "# Build pipeline with BalancedBaggingClassifier\n",
    "pipeline = ImbPipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"bagging\", BalancedBaggingClassifier(\n",
    "        estimator=base_xgb_classifier,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Define hyperparameter search space\n",
    "param_grid = {\n",
    "    \"bagging__n_estimators\": [10, 20, 30],\n",
    "    \"bagging__sampling_strategy\": [\"auto\", \"not minority\", \"majority\", 0.5, 0.75],\n",
    "    \"bagging__estimator__n_estimators\": [100, 200, 300],\n",
    "    \"bagging__estimator__max_depth\": [3, 5],\n",
    "    \"bagging__estimator__learning_rate\": [0.05, 0.1, 0.15],\n",
    "    \"bagging__estimator__subsample\": [0.8, 1.0],\n",
    "    \"bagging__estimator__colsample_bytree\": [0.8, 1.0],\n",
    "}\n",
    "\n",
    "# Setup CV and scorer\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scorer = make_scorer(f1_score, average=\"weighted\")\n",
    "\n",
    "# Initialize RandomizedSearchCV\n",
    "search = RandomizedSearchCV(\n",
    "    pipeline, param_distributions=param_grid,\n",
    "    n_iter=50,\n",
    "    scoring=scorer, cv=cv, verbose=2, random_state=42, n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "# Print best parameters and CV score\n",
    "print(f\"Best parameters: {search.best_params_}\")\n",
    "print(f\"Best CV weighted F1-score: {search.best_score_}\")\n",
    "\n",
    "# Evaluate on the test set\n",
    "y_pred = search.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Predict probabilities for AUC and log-loss calculation\n",
    "if hasattr(search.best_estimator_['bagging'], \"predict_proba\"):\n",
    "    y_pred_proba = search.predict_proba(X_test)[:, 1]\n",
    "else:\n",
    "    # Fallback if predict_proba not available\n",
    "    y_pred_proba = search.decision_function(X_test)\n",
    "    from sklearn.preprocessing import minmax_scale\n",
    "    y_pred_proba = minmax_scale(y_pred_proba)  # Scale scores to [0,1]\n",
    "\n",
    "# Calculate and print AUC score\n",
    "auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "print(f\"AUC Score: {auc_score:.4f}\")\n",
    "\n",
    "# Calculate and print Log-loss score\n",
    "logloss_score = log_loss(y_test, y_pred_proba)\n",
    "print(f\"Log-loss Score: {logloss_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb29f78b",
   "metadata": {},
   "source": [
    "## Pipeline 7: Bagging Ensemble with Scale-Weighted XGBoost\n",
    "\n",
    "\n",
    "### Key Points:\n",
    "- Bagging ensemble aggregates multiple XGBoost models trained on bootstrap samples to reduce variance and improve robustness.\n",
    "- Internal XGBoost weighting (`scale_pos_weight`) adjusts the loss function to penalise minority class misclassification proportionally.\n",
    "- Hyperparameter tuning spans ensemble size and key XGBoost parameters like tree depth, learning rate, and subsampling ratios.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e99896c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "Best parameters: {'bagging__n_estimators': 20, 'bagging__estimator__subsample': 1.0, 'bagging__estimator__n_estimators': 100, 'bagging__estimator__min_child_weight': 3, 'bagging__estimator__max_depth': 3, 'bagging__estimator__learning_rate': 0.05, 'bagging__estimator__colsample_bytree': 0.8}\n",
      "Best CV weighted F1-score: 0.6080566069699957\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.70      0.78      0.74        40\n",
      "         1.0       0.40      0.32      0.35        19\n",
      "\n",
      "    accuracy                           0.63        59\n",
      "   macro avg       0.55      0.55      0.55        59\n",
      "weighted avg       0.61      0.63      0.61        59\n",
      "\n",
      "AUC Score: 0.6079\n",
      "Log-loss Score: 0.6434\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_filtered, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Step 2: Calculate scale_pos_weight\n",
    "counter = Counter(y_train)\n",
    "scale_pos_weight = counter[0.0] / counter[1.0] if counter[1.0] != 0 else 1\n",
    "\n",
    "# Step 3: Define base XGBoost with scale_pos_weight\n",
    "base_xgb_classifier = XGBClassifier(\n",
    "    random_state=42,\n",
    "    eval_metric=\"logloss\",\n",
    "    scale_pos_weight=scale_pos_weight\n",
    ")\n",
    "\n",
    "# Step 4: Build pipeline with bagging\n",
    "pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"bagging\", BaggingClassifier(\n",
    "        estimator=base_xgb_classifier,\n",
    "        n_estimators=10,\n",
    "        max_samples=1.0,\n",
    "        max_features=1.0,\n",
    "        bootstrap=True,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Step 5: Hyperparameter space\n",
    "param_grid = {\n",
    "    \"bagging__n_estimators\": [10, 20],\n",
    "    \"bagging__estimator__n_estimators\": [100, 200],\n",
    "    \"bagging__estimator__max_depth\": [3, 5],\n",
    "    \"bagging__estimator__learning_rate\": [0.05, 0.1],\n",
    "    \"bagging__estimator__subsample\": [0.8, 1.0],\n",
    "    \"bagging__estimator__colsample_bytree\": [0.8, 1.0],\n",
    "    \"bagging__estimator__min_child_weight\": [1, 3],\n",
    "}\n",
    "\n",
    "# Step 6: Setup CV and scorer\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scorer = make_scorer(f1_score, average=\"weighted\")\n",
    "\n",
    "# Step 7: RandomizedSearchCV\n",
    "search = RandomizedSearchCV(\n",
    "    pipeline, param_distributions=param_grid, n_iter=50,\n",
    "    scoring=scorer, cv=cv, verbose=2, random_state=42, n_jobs=-1\n",
    ")\n",
    "\n",
    "# Step 8: Fit on training data\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "# Step 9: Print best parameters and CV score\n",
    "print(f\"Best parameters: {search.best_params_}\")\n",
    "print(f\"Best CV weighted F1-score: {search.best_score_}\")\n",
    "\n",
    "# Step 10: Evaluate on test data\n",
    "y_pred = search.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Step 11: Predict probabilities for AUC and log-loss\n",
    "if hasattr(search.best_estimator_['bagging'], \"predict_proba\"):\n",
    "    y_pred_proba = search.predict_proba(X_test)[:, 1]\n",
    "else:\n",
    "    y_pred_proba = search.decision_function(X_test)\n",
    "    from sklearn.preprocessing import minmax_scale\n",
    "    y_pred_proba = minmax_scale(y_pred_proba)\n",
    "\n",
    "# Step 12: Calculate and print AUC and log-loss\n",
    "auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "print(f\"AUC Score: {auc_score:.4f}\")\n",
    "\n",
    "logloss_score = log_loss(y_test, y_pred_proba)\n",
    "print(f\"Log-loss Score: {logloss_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0544570b",
   "metadata": {},
   "source": [
    "## Pipeline 8: Ensemble Learning with XGBoost and Bagging \n",
    "\n",
    "* **Sets up a Robust Pipeline**: An `ImbPipeline` is created to handle the entire machine learning workflow in a single object. This ensures data preprocessing steps (imputation, scaling) and the classification model are applied consistently.\n",
    "* **Implements Bagging**: The `BalancedBaggingClassifier` acts as the core of our ensemble. It trains multiple XGBoost models on different, randomly sampled subsets of the training data. Crucially, it automatically balances the classes within each subset, solving the problem of class imbalance without needing a separate oversampling step like ADASYN.\n",
    "* **Tunes Hyperparameters**: `RandomizedSearchCV` is used to efficiently search for the best combination of parameters for both the bagging ensemble (e.g., `n_estimators`, or the number of XGBoost models to train) and the individual XGBoost models within it (e.g., `max_depth`, `learning_rate`).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd81ddf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ \\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\model_selection\\_validation.py:528: FitFailedWarning: \n",
      "70 fits failed out of a total of 250.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "70 fits failed with the following error:\n",
      "joblib.externals.loky.process_executor._RemoteTraceback: \n",
      "\"\"\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ \\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\joblib\\_utils.py\", line 72, in __call__\n",
      "    return self.func(**kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ \\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\joblib\\parallel.py\", line 598, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ \\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\utils\\parallel.py\", line 139, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ \\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\ensemble\\_bagging.py\", line 197, in _parallel_build_estimators\n",
      "    estimator_fit(X_, y_, **fit_params_)\n",
      "  File \"C:\\Users\\ \\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ \\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\imblearn\\pipeline.py\", line 518, in fit\n",
      "    Xt, yt = self._fit(X, y, routed_params, raw_params=params)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ \\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\imblearn\\pipeline.py\", line 440, in _fit\n",
      "    X, y, fitted_transformer = fit_resample_one_cached(\n",
      "                               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ \\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\joblib\\memory.py\", line 312, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ \\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\imblearn\\pipeline.py\", line 1336, in _fit_resample_one\n",
      "    X_res, y_res = sampler.fit_resample(X, y, **params.get(\"fit_resample\", {}))\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ \\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\imblearn\\base.py\", line 202, in fit_resample\n",
      "    return super().fit_resample(X, y, **params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ \\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ \\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\imblearn\\base.py\", line 101, in fit_resample\n",
      "    self.sampling_strategy_ = check_sampling_strategy(\n",
      "                              ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ \\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\imblearn\\utils\\_validation.py\", line 571, in check_sampling_strategy\n",
      "    _sampling_strategy_float(sampling_strategy, y, sampling_type).items()\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ \\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\imblearn\\utils\\_validation.py\", line 430, in _sampling_strategy_float\n",
      "    raise ValueError(\n",
      "ValueError: The specified ratio required to generate new sample in the majority class while trying to remove samples. Please increase the ratio.\n",
      "\"\"\"\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ \\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\ \\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ \\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\imblearn\\pipeline.py\", line 526, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **last_step_params[\"fit\"])\n",
      "  File \"C:\\Users\\ \\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ \\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\imblearn\\ensemble\\_bagging.py\", line 337, in fit\n",
      "    return super().fit(X, y)\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ \\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ \\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ \\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\ensemble\\_bagging.py\", line 389, in fit\n",
      "    return self._fit(X, y, max_samples=self.max_samples, **fit_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ \\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\imblearn\\ensemble\\_bagging.py\", line 352, in _fit\n",
      "    return super()._fit(X, y, self.max_samples)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ \\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\ensemble\\_bagging.py\", line 532, in _fit\n",
      "    all_results = Parallel(\n",
      "                  ^^^^^^^^^\n",
      "  File \"C:\\Users\\ \\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\utils\\parallel.py\", line 77, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ \\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\joblib\\parallel.py\", line 2007, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "                                                ^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ \\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\joblib\\parallel.py\", line 1650, in _get_outputs\n",
      "    yield from self._retrieve()\n",
      "  File \"C:\\Users\\ \\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\joblib\\parallel.py\", line 1754, in _retrieve\n",
      "    self._raise_error_fast()\n",
      "  File \"C:\\Users\\ \\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\joblib\\parallel.py\", line 1789, in _raise_error_fast\n",
      "    error_job.get_result(self.timeout)\n",
      "  File \"C:\\Users\\ \\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\joblib\\parallel.py\", line 745, in get_result\n",
      "    return self._return_or_raise()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ \\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\joblib\\parallel.py\", line 763, in _return_or_raise\n",
      "    raise self._result\n",
      "ValueError: The specified ratio required to generate new sample in the majority class while trying to remove samples. Please increase the ratio.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\ \\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\model_selection\\_search.py:1108: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.62938747        nan 0.605657   0.62829756\n",
      " 0.64254157        nan 0.60577127        nan        nan 0.62682452\n",
      " 0.6199054         nan 0.61972428        nan        nan 0.62057799\n",
      " 0.6192555         nan 0.60548254 0.62832858 0.61129798        nan\n",
      "        nan 0.62076134 0.6233256  0.6251323  0.61800686 0.63275376\n",
      " 0.60736312 0.61163831 0.63452024 0.63208127 0.60533736 0.62829756\n",
      " 0.63275376 0.61294419 0.61006985 0.6077495  0.60843189 0.649528\n",
      " 0.63976763 0.60495953 0.64254157 0.62493011        nan 0.65184106\n",
      " 0.61838679        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'bagging__sampling_strategy': 'not minority', 'bagging__n_estimators': 10, 'bagging__estimator__subsample': 0.8, 'bagging__estimator__n_estimators': 200, 'bagging__estimator__max_depth': 3, 'bagging__estimator__learning_rate': 0.05, 'bagging__estimator__colsample_bytree': 0.8}\n",
      "Best CV weighted F1-score: 0.6518410594912083\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.67      0.65      0.66        40\n",
      "         1.0       0.30      0.32      0.31        19\n",
      "\n",
      "    accuracy                           0.54        59\n",
      "   macro avg       0.48      0.48      0.48        59\n",
      "weighted avg       0.55      0.54      0.55        59\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Assume X_filtered and y are already defined\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_filtered, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# 2. Define the base XGBoost model\n",
    "base_xgb_classifier = XGBClassifier(\n",
    "    random_state=42,\n",
    "    eval_metric=\"logloss\"\n",
    ")\n",
    "\n",
    "# 3. Build the full imbalanced-learn pipeline\n",
    "pipeline = ImbPipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"bagging\", BalancedBaggingClassifier(\n",
    "        estimator=base_xgb_classifier,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "# 4. Define a refined hyperparameter search space\n",
    "param_grid = {\n",
    "    \"bagging__n_estimators\": [10, 20, 30],\n",
    "    \"bagging__sampling_strategy\": [\"auto\", \"not minority\", \"majority\", 0.5, 0.75],\n",
    "    \"bagging__estimator__n_estimators\": [100, 200, 300],\n",
    "    \"bagging__estimator__max_depth\": [3, 5],\n",
    "    \"bagging__estimator__learning_rate\": [0.05, 0.1, 0.15],\n",
    "    \"bagging__estimator__subsample\": [0.8, 1.0],\n",
    "    \"bagging__estimator__colsample_bytree\": [0.8, 1.0],\n",
    "}\n",
    "\n",
    "# 5. Set up cross-validation and scorer\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scorer = make_scorer(f1_score, average=\"weighted\")\n",
    "\n",
    "# 6. Initialise RandomizedSearchCV\n",
    "search = RandomizedSearchCV(\n",
    "    pipeline, param_distributions=param_grid,\n",
    "    n_iter=50, scoring=scorer, cv=cv, verbose=2, random_state=42, n_jobs=-1\n",
    ")\n",
    "\n",
    "# 7. Fit the model\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "# 8. Print best parameters and CV score\n",
    "print(f\"Best parameters: {search.best_params_}\")\n",
    "print(f\"Best CV weighted F1-score: {search.best_score_}\")\n",
    "\n",
    "# 9. Evaluate on the test set\n",
    "y_pred = search.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5404fe46",
   "metadata": {},
   "source": [
    "## Conclusion Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2e6940",
   "metadata": {},
   "source": [
    "Sampling Strategy: ADASYN sampling strategy tuning improved the minority-class F1-score in one pipeline setup. Keeping \"adasyn__sampling_strategy\": \"auto\" or values close to 1.0 seems beneficial.\n",
    "\n",
    "Hyperparameter Range Refinement: Refining the hyperparameters to smaller ranges around previously found best values (especially max_depth 3-5, learning_rate 0.005-0.02, and subsample/colsample 0.6-0.9) yielded modest gains in F1 and minority recall.\n",
    "\n",
    "Custom Minority F1 Scorer: Using a make_scorer(f1_score, pos_label=1.0) focused evaluation on the minority class provided better-focused tuning. This is critical given the class imbalance and objective.\n",
    "\n",
    "Scale Pos Weight: Explicit use of scale_pos_weight to counter imbalance showed limited impact on minority class recall in your experiments.\n",
    "\n",
    "Ensemble Techniques (Bagging/BalancedBagging): Did improve weighted F1 slightly but did not boost minority recall or sensitivity convincingly. Also, failures in some fits with ensembles indicate some instability or incompatibility with certain hyperparameter combos.\n",
    "\n",
    "Pipeline Simplification: Avoiding PCA and sticking with imputation, scaling, ADASYN, and carefully tuned XGBoost gave more stable, moderately better results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
